# Main script generate SciTextures  dataset

import os
import tools.MainFunctions as MF

import tools.VisualQuestion as VQ
import json_pkl
import shutil
import re


##########################################Suggest models and simulations to generate patterns and implement them in code ###################################################################################3
def generate_scitextures(dataset_dir, query_dir, number_of_new=10, number_of_code_fix_retry=2, recheck_originality=True, idea_model="", code_model="", check_model="", model="gpt-5"):

    if idea_model == "": idea_model = model # model used to suggest ideas
    if code_model == "": code_model = model # model used for implement and debug code
    if check_model == "": check_model = model  # model used for checking the model generated by the coding model and find errors

    # its essential that benchmark_dir will be in relative path (it will later be converted into import)
    data_file = dataset_dir + "//data.pkl" # main data and log file  for the dataset contain all data code/descriptions
    data_file_back = dataset_dir + "//data_back.pkl"


    if not os.path.exists(dataset_dir): os.mkdir(dataset_dir) # output dir

    #=========load queries and prompts==========================
    dt={'qr': {}, "messages":[]} # Queries for the LVLMs (GPT)
    for fl in os.listdir(query_dir): # load queries from file
        dt['qr'][fl] = open(query_dir + "//" + fl, "r", encoding="utf-8").read()
    ######################################Suggest bencmarks################################################################################


    data_file_loaded=False # Loaded datafile
    if os.path.isfile(data_file):  # Load data file for the dataset
        print("\n\n\nLoad file\n\n\n")
        qr=dt['qr']
        dt=json_pkl.read_pkl(data_file)
        dt['qr']=qr
        data_file_loaded=True
        #............Removing unused............................................................................
        to_remove=[]
        if not 'benchmarks' in dt: dt['benchmarks'] = {}
        for bname in dt['benchmarks']:
            ent = dt['benchmarks'][bname]
            if 'full_overlap' in ent and ent['full_overlap'] == True and 'code' not in ent:
                to_remove.append(bname)
        for bname in to_remove:
                print("Removing:",bname)
                del dt['benchmarks'][bname]
        #,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,


#===========================Suggest new ideas and models==================================================================

    if  number_of_new>0: # Suggest ideas and models that
        print("\n\n\nSuggest Model for Patterns and Texture generations\n\n\n")
        txt=dt['qr']['suggest_benchmarks'].replace("@@@number_of_new@@@",str(number_of_new))
        if data_file_loaded: # prepare queries display exisiting models/methods in the dataset as part of the query (to avoid repetition)
            txt+=dt['qr']['add_suggestions']+ "\n Previous suggested methods: ["
            for ky in dt['benchmarks']:
                txt+=str(ky)+","
            txt+"]"

        resp,dt= VQ.get_reponse(dt, text=txt, model=idea_model) # suggest ideas for models and textures

#-----------------Convert ideas/suggestion to json dictionaries------------------------------------------------------------------------

        dt['benchmarks_text']=resp
        dt['messages'].append({"role": "user", "content":dt['qr']['suggestions_to_json']})
        benchmark_dic,dt = VQ.get_reponse(dt, messages=dt['messages'][-3:],as_json=True, model=idea_model)
        if 'benchmarks' in dt:
           for ky in benchmark_dic:
                if ky not in dt['benchmarks']:
                    dt['benchmarks'][ky]=benchmark_dic[ky]
                    print(benchmark_dic[ky])
                else:
                    print("error ", ky, "already exists")
        else:
           dt['benchmarks']=benchmark_dic
        json_pkl.save_pkl(dt,data_file)

#=================================Generate code for each suggested model separately ===================================================================================================
    if not 'benchmarks' in dt:
           dt['benchmarks'] = {}
    for bname in dt['benchmarks']:
        print("benchmark",bname)
        ent=dt['benchmarks'][bname]

        bdesc = ent["description"]

#---------------------if manual checked applied if entries failed manual inspection redo the code (manual inspection is seperate script)--------------------------------------------------------
        if "checked" in ent: # this mean this entery passed manual inspection
            if  ent['checked'] == 'pass': continue # if checked and passed continue
            if "code" in ent: # if the code failed manual inspection remove it
                ent["old_code"]=ent["code"]
                del ent["code"]
                del ent['checked']
                ent["redo"]=True
#------------------------------------------------------------------------------------------------------------

        if ('code' in ent) and ('code verified' in ent['code'] or 'finished_and_failed' in ent['code']): continue # if code already created and verified skip
        if 'full_overlap' in ent: continue # if overlap existing method continue

# -------------------------------Recheck idea originality optional recheck the idea is not already in the list of implemented models in the dataset -------------------------------------------------------------------------------------------------

        if recheck_originality and (not 'code' in ent) :
            print("Rechecking Checking originaliy against existing dataset entries")
            benchmark_list = "\n Previous methods:\n ["
            cnt_ex = 0  # count previous benchmar
            for ky in dt['benchmarks']:
                if bname == ky: continue
                if 'code' not in dt['benchmarks'][ky]: continue
                benchmark_list += str(ky) + "," + ky
                cnt_ex += 1
            benchmark_list += "]\n"
            if cnt_ex > 0:
                chek_query = dt['qr']['Check_originality'].replace("**benchmark_name**", bname)
                chek_query = chek_query.replace("**existing benchmark**", benchmark_list)
                overlap_check, dt = VQ.get_reponse(dt, text=chek_query, as_json=True, model=model)
                ent["overlap"] = overlap_check
                if ("match" in overlap_check) and overlap_check["match"] == "yes":
                    print("Similar benchmarks Exists", ent["overlap"] )
                    ent['full_overlap'] = True
                    json_pkl.save_pkl(dt, data_file)
                    continue
        # ------------------------------------Implement method to code--------------------------------------------------------------------------------------

        code_query=dt['qr']['implement_code'].replace("**benchmark_name**",bname).replace("**becnhmark_description**",bdesc)
        if (not 'code' in ent) or ('Succeed' not in ent['code']):# or ent['code']['Succeed']=='no':
                    print("\n\n\nWrite code for:" + bname + "\n\n\n")
                    for gg in range(10):

                        code_dic, dt = VQ.get_reponse(dt, text=code_query, as_json=True, model=code_model)
                        if ('code' in code_dic) and  ('Succeed' not in code_dic) and len(code_dic['code'])>100: code_dic['Succeed']="yes"
                        if ('Succeed' not in code_dic):
                      #       code_query+="\n\nIts very important that the output will be in the precise format described aboce\n\n"
                             continue
                        dt['benchmarks'][bname]['code'] = code_dic
                        dt['benchmarks'][bname]['code']['query'] = code_query
                        break
                    json_pkl.save_pkl(dt, data_file)
                    json_pkl.save_pkl(dt, data_file_back)

    #=============================Run Debug and validate code=========================================================

        #try:
        if ent['code']['Succeed']=='no': continue
        # except:
        #     d=3
        #     x=fsfs
        if 'code verified' not in ent['code'] or ent['code']['code verified']==False:
            print("\n\n\nTest and validate  code for:" + bname + "\n\n\n")
            sname = f"{re.sub(r'[^a-zA-Z]', '_', bname)}"
            dt['benchmarks'][bname]['simple name']= sname
            benchdir= dataset_dir + "//" + sname + "//"
            code_path = benchdir+"generate.py"
            bench_sample_dir = benchdir + "//textures//"
            dt['benchmarks'][bname]['dir']=benchdir
            #-----Validation code run the generator and test results----------------------------
            testing_code_str = (
                        "\nimport importlib\n" +
                        "\nimport " + MF.path_to_import(code_path) + " as generate\n" +
                        #code_path.replace("//",".").replace(".py","").replace("/",".").replace("..",".") + " as generate\n"+
                        "\nimportlib.reload(generate)"
                        "\noutdir = '" + bench_sample_dir + "'" +
                        dt['qr']["run_line"]
                        #"\ngenerate.generate_benchmark (texture_dir,shape_dir,outdir, num_samples=30)\n"
            )
            for kk in range(number_of_code_fix_retry+100):
                if not os.path.exists(dataset_dir): os.mkdir(benchdir)
                code = dt['benchmarks'][bname]['code']['code']
            ###    code_generation_talk=dt['messages'][-2:]
                ln=len(dt['messages'])


               #-------------------Run code and debug---------------------------------------------------------------------------
                code_verified, path, test_dir, code, captured_stdout, messages = (
                MF.run_debug_code(messages =dt['messages'][-2:], code=code,
                                  code_dir=benchdir, functions_and_var={}, codefilename="generate.py",
                                  testing_code=testing_code_str, task_description=bdesc, time_out = 1800, rechek_code=False, model=code_model))


                with open(test_dir+"//Description.txt","w") as fl: fl.write(bdesc) # write benchmark description
                if  "overlap" in ent:
                     json_pkl.save_json(ent["overlap"],test_dir+"//overlap.txt")

   #=======================================================================================================================
                #...........Recheck codes and identify errors .................................................

                #...............Run of code and collect tesults...................................
                if not code_verified:
                    dt['messages']=dt['messages'][:ln]
                    #continue
                    break

                txt="***Original Code Generation task:***\n"+code_query+"\n\n\n"
                txt+="***Generated Code***:\n"+code+"\n\n\n"
                txt+"***Some info on generated images created by running the code***"
                uniform_error=False # images that are completely uniform are errors
                dark_error=False # images that are to dark are errors
                sim_error=False # images that are very similar to each other are problemetic
                for xx,fl in enumerate(os.listdir(bench_sample_dir)):
                    if ".png" in fl:
                        import cv2
                        im=cv2.imread(bench_sample_dir+"//"+fl,0)
                        txt+="\nImage: "+fl+"  Size: "+str(im.shape[0:2])+" Pixels values range: "+str(im.min())+" to "+str(im.max())
                        if  im.max()-im.min()<15: # unifor,
                                   uniform_error=True
                                   txt += "\n***the values of pixels distrubtion is too uniform:" + str(
                                       im.max()) + "  which will make the image practically uniform when viewed***\n"

                        if im.max()<10:
                                   dark_error=True
                                   txt += "\n***The values of  pixesl in the images are all below "+str(im.max)+" Out of 255 make the image very dark\n"
                        if xx>1:
                            if (prev_im.ndim==im.ndim and prev_im.shape[0]==im.shape[0]
                                    and  prev_im.shape[1]==im.shape[1]
                                    and  ((im-prev_im).__abs__().mean()<0.001 or (im-prev_im).max()<9)):
                                txt += "\n***This image is identical very similar to the previous one, the script should generated visually different images.***\n"
                                sim_error=True
                        prev_im = im.copy()
                        if xx>6: break
                #    if dark_error or uniform_error:

                txt += "\n\n\n***Your task:***\n"+dt['qr']['check_code']
                if dark_error:
                    txt +="\n\nAt least is in some images the image is way too dark,  making them practically black, this is not good can you avoid this?\n"
                if uniform_error:
                    txt +="\n\nAn issue you must fix is that some of the images are just uniform (the same or narrow set of values for all pixels) making them practically uniform, this is not good can you avoid it?\n"
                if sim_error:
                    txt +="\n\nSome or all of the generated images are very similar to each other. This is not good. Can you make all images clearly different from one  another\n"
                 # limit the number of attempt to fixe the code otherwise you might be stack in endless loop
                if kk >= number_of_code_fix_retry and not (uniform_error or dark_error or sim_error): break # give it more tries if you identify obvious errors in output
                if kk >= number_of_code_fix_retry+3: break

                verify_query=txt
                print(txt)
                code_dic, dt = VQ.get_reponse(dt, text=verify_query, as_json=True, model=check_model) # Ask the checker LLM to identify and correct errors
    #            print(dt['benchmarks'][bname]['code']['discussion'])
                if code_dic['corrections']=='yes':# and len(code_dic['code'])>0:
                    print("\\n\n\n****************************************************************************\n\n\nCode correction found:",code_dic)
                    del dt['benchmarks'][bname]['code']
                    dt['benchmarks'][bname]['code'] = code_dic
                    dt['benchmarks'][bname]['code']['Succeed'] = 'yes'
                    dt['benchmarks'][bname]['code']['query'] = code_query
                    dt['benchmarks'][bname]['code']['fixing_query'] = verify_query

                else:
                    break


                #......................................

 # Finish generatin one entry  Write to file

            dt['benchmarks'][bname]['code']['code verified'] = code_verified
            if not code_verified: dt['benchmarks'][bname]['code']['finished_and_failed']=True
          ###  dt['benchmarks'][bname]['code']['code'] = code
            json_pkl.save_pkl(dt, data_file)
            json_pkl.save_pkl(dt, data_file_back)
            print("\n\n\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n\nFinished benchmark ",bname,"\nverified ",code_verified,"\n\n\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n\n")


################################################################################################################################################################

if __name__=="__main__":
        query_dir = "queries_prompts//queries_textures_pattern_generation_conseravative_1//" # Folder where prompts/queries are save
        outputdir = "Scitextures//" # output folder where the generated dataset will be saved,
        ## its important that this will be subfolder of the code folder and given
        # in relative path  as files in this folder will be imported to the code


        data_file = outputdir + "//data.pkl" # Data file contain all data in the dataset
        number_of_code_fix_retry = 1  # 3 # number of times to try to fix code before accepting the final results (note that over analyzing the code often make the results wors
        recheck_originality=True #True # double check the idea is in dataset also identify related ideas
        model = "gpt-5"#grok-4-fast-reasoning"#"Qwen/Qwen2.5-VL-72B-Instruct"#"claude-sonnet-4-5-20250929" #"grok-4-fast-reasoning"#"gemini-2.5-flash"#"Qwen/Qwen2.5-VL-72B-Instruct"#"gpt-5"
        for kk in range(100):
            # if kk==0:
            #     number_of_new= 0 # allow the method to finish existing un implemented ideas
            # else:
            #     number_of_new= 4 # Number of new ideas to suggest in each round (
            number_of_new = 3  # Number of new ideas to suggest in each round (
            generate_scitextures(dataset_dir=outputdir, query_dir=query_dir, number_of_new=number_of_new, number_of_code_fix_retry=number_of_code_fix_retry, recheck_originality=recheck_originality, model=model)

